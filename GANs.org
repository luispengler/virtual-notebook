#+TITLE: Machine Learning
#+DESCRIPTION: Machinen Learning
#+SETUPFILE: https://luispengler.github.io/site/org-html-themes/org/theme-readtheorg.setup
#+OPTIONS: num:nil ^:{} toc:1

* Introduction
So, my mentor gave me another task and now I need to get the most out of the book "GANs in action"
+ My mentor is [[https://scholar.google.com/citations?user=cHCDiIYAAAAJ][Dr. Abhijit Sen]], and we are starting research through the project [[https://www.ndeavours.org/][Ndeavours]]
+ Keep tuned for updates into what we are doing - We will do stuff around the same topic of this notes page! (GANs :)
+ As always, this page is mostly copying the main parts of each chapters and topics, something I call "notes"
+ Another observation is that I am not enterily new to the concept of GANs, so I may not give much details in the notes

* Chapter One - Introduction to GANs
** What are Generative Adversarial Networks?
+ They are a class of machine learning techniques that consist of two simultaneously trained models: one (the Generator) trained to generate fake data, and the other (the Discriminator) trained to discern the fake data from real examples
+ The word generative tells the overall purpose of this model, which is to generate new data
+ The word adversarial The generator and discriminator networks are trying to outwit each other
+ And the word networks indicates the class of machine learning models most commonly used to represent the generator and discriminator, which are neural networks
+ Depending on the complexity, we might choose between three neural networks: simple feed-forward neural networks, convulutional neural networks, U-Net
** How do GANs work?
+ The table below from the book answers this question very well
|        | Generator                                                                         | Discriminator                                                                                   |
| Input  | A vector of random numbers                                                        | Real examples coming from the training dataset, and fake examples coming from the generator     |
| Output | Fake examples that strive to be as conving as possible                            | Predicted probability that the input example is real                                            |
| Goal   | Generate fake data that is indistinguishable from members of the training dataset | Distinguish between the fake examples from the Generator and the real examples from the dataset |
** GAN training
*** Training the Discriminator
+ Take a random real example x from the traning dataset
+ Get a new random noise vector z and, using  the Generator network, synthesize a fake example x*
+ Use the Discriminator network to classify x and x*
+ Computer the classification erros and backpropagate the total error to update the Discriminator weights and biases, seeking to minimize the classification errors
*** Training the Generator
+ Get a new random noise vector z and, using the Generator network, synthesize a fake example x*
+ Use the Discriminator network to classify x*
+ Computer the classification error and backpropagate the error to update the Generator weights and biases, seeking to maximize the Discriminator's error
*** Reaching equilibrium
+ The GAN setting is a zero-sum game, a situation in which one player's gains equal the other player's losses. When one player improves by a certain amount, the other player worsens by the same amount
+ All zero-sum games have a Nash equilibrium, a point at which neither player can improve their situation or payoff by changing their actions
+ This Nash equilibrium is reached when the generator produces fake examples that are indistinguishable from the real data in the training dataset, and the discriminator can at best randomly guess whether a particular example is real or fake
+ With equilibrium achieved, GAN is said to have converged
+ In practice, it is nearly impossible to find the Nash equilibrium for GANs because of the immense complexities involved in reaching convergence in nonconvex games
** Why study GANs?
+ GANs have extensive applications across many different sectors, such as fashion, medicine, and cybersecurity

* Chapter Two - Into to generative modeling with autoencoders
** Introduction to generative modeling
#+BEGIN_QUOTE
We start with a prescription of what we want to produce and get the image at the other end of the transformations. That is generative modeling in its simplest, most informal form;
#+END_QUOTE
+ This prespecription described in the quote above is said to be living in a latent space, and it serves as an inspiration so that we do not always get the same output, \[x^*\].
+ The random noise vector from chapter 1 is often referred to as sample from the latent space.
+ This so called latent space is a simpler, hidden representation of a data point. Simpler just means with fewer dimensions.
** How do autoencoders function on a high level?
#+BEGIN_QUOTE
DEFINITION: The latent space is the hidden representation of the data. Rather than expressing words or images (for example, machine learning engineer in our example, or JPEG codec for images) in their uncompressed versions, an autoencoder compresses and clusters them based on its understanding of the data.
#+END_QUOTE
** What is an autoencoder made of?
+ Very simply, it has three steps: Encoder or compression network (step 1), latent space of size z (step 2), and output or decompression or reconstruction (step 3). Before the steps we have x, an image as vector of size y, and after we have x*, output as vector of size y.
** Unsupervised learning
#+BEGIN_QUOTE
DEFINITION: Unsupervised learning is a type of machine learning in which we learn from the data itself without additional labels as to what this data means. Clustering, for example, is unsupervised—because we are just trying to discover the underlying structure of the data; but anomaly detection is usually supervised, as we need human-labeled anomalies.
#+END_QUOTE
*** Generation using an autoencoder
+ For generation, we cut off the encoder part and use only the latent space and the decoder
#+BEGIN_QUOTE
Because we know from training where our examples get placed in the latent space, we can easily generate examples similar to the ones that the model has seen. Even if not, we can easily iterate or grid-search through the latent space to determine the kinds of representations that our model can generate.
#+END_QUOTE
** Code is life
+ Now Imma head to my jupyter notebooks :)
+ Summary of my code: We are going to create an object, generator or decoder, that can use  the predict() method to generate new examples of handwritten digits, given an input seed, which is just the latent space vector
+ I didn't understand much of the math behind the distributions and why we are using it for the encoder-decoder
*** Problems
+ VAE picks the safe path and makes the background blurry by chooseing a safe pixel value, which minimzes the loss, but does not provide good images
** Summary
+ Autoencoders on a high level are composed of an encoder, a latent space, and a decoder. An autoencoder is trained by using a common objective function that measures the distance between the reproduced and the original data
+ Autoencoders have many applications and can also be used as a generative model. In practice, this tends not to be their primary use because other methods, especially GANs, are better at the generative task.
+ We can use Keras (a high-level API for TensorFlow) to write a simple variational autoencoder that produces handwritten  digits
+ VAEs have limitations that motivate us to move on to GANs

* Chapter Three - Your fist GAN: Generating handwritten digits
** Foundations of GANs: Adversarial training
#+BEGIN_QUOTE
Formally, the Generator and the Discriminator are represented by differentiable functions, such as neural networks, each with its own cost function. The two networks are trained by backpropagation by using the Discriminator’s loss. The Discriminator strives to minimize the loss for both the real and the fake examples, while the Generator tries to maximize the Discriminator’s loss for the fake examples it produces.
#+END_QUOTE
+ For a computer, an image is just a matrix of values. If it is grascale than it is two-dimensional, if it is RGB thant it is three-dimensional
*** Cost functions
+ GANs differ from traditional neural networks in two key aspects. One is that in a conventional neural network, the cost function \[J\] is defined exclusively in terms of its own trainable parameters (the weights and biases, which we put together in a letter called theta \[\theta\] as in \[J(\theta)\]. However GANs are made up of two networks whose cost functions are dependent on both of the network's parameters (the generator (G) and the discriminator (D)) and are defined like this \[J^{(G)}(\theta^{(G)},\theta^{(D)})\] for the Generator's cost function, and \[J^{(D)}(\theta^{(G)},\theta^{(D)})\] for the Discriminator's cost function.
+ The second difference is that while a traditional neural network can tune all its parameters during the training process, as GANs consist of two different network, each only can tune its own weights and biases. In other words, the Generator can only tune \[\theta^{(G)}\] and the Discriminator can only tune \[\theta^{(D)}\]
*** Training process
+ The training of a traditional neural network is an optimization problem, where finding a set of parameters such that moving to any neighboring point in the paramters would increase the cost and would go against our minimalizatioon of the cost function.
+ However, we can better define GAN training as a game than an optimization
#+BEGIN_QUOTE
Training GANs successfully requires trial and error, and although there are best practices, it remains as much an art as it is a science.
#+END_QUOTE
*** Conflicting objectives
#+BEGIN_QUOTE
The Discriminator’s goal is to be as accurate as possible. For the real examples x, D(x) seeks to be as close as possible to 1 (label for the positive class). For fake examples x*, D(x*) strives to be as close as possible to 0 (label for the negative class). The Generator’s goal is the opposite. It seeks to fool the Discriminator by producing fake examples x* that are indistinguishable from the real data in the training dataset. Mathematically, the Generator strives to produce fake examples x* such that D(x*) is as close to 1 as possible.
#+END_QUOTE
** Summary
+ GANs consist of two networks: the Generator (G) and the Discriminator (D), each with its own loss function: \[J^{(G)}(\theta^{(G)},\theta^{(D)})\] and \[J^{(D)}(\theta^{(G)},\theta^{(D)})\] respectively.
+ During training, the Generator and the Discriminator can tune only their own parameters: \[\theta^{(G)}\] and \[\theta^{(D)}\], respectively.
+ The two GAN networks are trained simultaneously via a game-like dynamic. The Generator seeks to maximize the Discriminator’s false-positive classifications (classifying a generated image as real), while the Discriminator seeks to minimize its false-positve and false-negative classifications.

* Chapter Four - Deep Convolutional GAN


* Resources
+ Book: GANs IN ACTION - Deep Learning with Generative Adversarial Networks
+ Deep Learning with Python - François Chollet
